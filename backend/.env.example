# Environment Configuration
# Copy this file to .env and adjust values as needed

# ========== Whisper API Configuration ==========
# Containerized Whisper service endpoint
WHISPER_API=http://localhost:8207
WHISPER_MODEL=large-v3

# Legacy settings (for local transcriber fallback)
WHISPER_DEVICE=cuda
WHISPER_COMPUTE_TYPE=float16

# ========== Ollama LLM Configuration ==========
OLLAMA_API=http://localhost:11434
OLLAMA_MODEL=qwen3:8b
OLLAMA_TEMPERATURE=0.3
OLLAMA_NUM_PREDICT=4096

# LLM Processing
LLM_ENABLED=true
LLM_TIMEOUT=300

# Two-pass batch processing
BATCH_SIZE_PASS1=8
BATCH_SIZE_PASS2=20

# API retry settings
API_MAX_RETRIES=3
API_RETRY_DELAY=2.0

# ========== Pipeline Settings ==========
MAX_DOWNLOAD_WORKERS=3
TRANSCRIBE_WORKERS=1

# ========== Output Paths ==========
OUTPUT_BASE_DIR=./transcripts
TEMP_AUDIO_DIR=./temp_audio

# ========== Server Settings ==========
HOST=0.0.0.0
PORT=8000
CORS_ORIGINS=["http://localhost:5173", "http://127.0.0.1:5173"]
